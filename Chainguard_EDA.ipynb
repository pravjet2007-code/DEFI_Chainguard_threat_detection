{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71d0e071",
   "metadata": {},
   "source": [
    "\n",
    "#  Chainguard Threat Intelligence — Demonstration Notebook (Ethereum Sample)\n",
    "\n",
    "This notebook uses your uploaded **`ethereum_sample_data.csv`** and walks through:\n",
    "\n",
    "1. **Transaction input**\n",
    "2. **Hashing** (salted SHA-256)\n",
    "3. **Feature extraction** (value, fees, counts, z-score, temporal features)\n",
    "4. **Risk scoring** (uses trained model if present; heuristic fallback otherwise)\n",
    "5. **Classification & reporting** (Top‑100, Watchlist, Deep‑Dive)\n",
    "6. **Rich Visualizations** for risk scores\n",
    "\n",
    "> Run cells top-to-bottom. Outputs are saved next to the notebook so your Streamlit app can reuse them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f7b3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR: c:\\Users\\pravin sharma\\Downloads\n",
      "DATA_PATH exists: False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import hashlib\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', 160)\n",
    "\n",
    "BASE_DIR = os.path.abspath('.')\n",
    "DATA_PATH = r\"C:\\Users\\pravin sharma\\OneDrive\\Documents\\BugSlayers_ChainGuard\\Data\"  # uploaded dataset\n",
    "MODEL_DIR = os.path.join(BASE_DIR, 'models')\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, 'chainguard_if_model.pkl')\n",
    "SCALER_PATH = os.path.join(MODEL_DIR, 'chainguard_scaler.pkl'\n",
    "# Output artifact paths (compatible with your app)\n",
    "OUT_ALL_SCORED = os.path.join(BASE_DIR, 'all_scored_transactions.csv')\n",
    "OUT_TOP100 = os.path.join(BASE_DIR, 'top_100_risky_transactions.csv')\n",
    "OUT_WATCHLIST = os.path.join(BASE_DIR, 'watchlist_accounts.csv')\n",
    "OUT_DEEPDIVE = os.path.join(BASE_DIR, 'watchlist_deep_dive_report.csv')\n",
    "\n",
    "RISK_THRESHOLD = 95\n",
    "WATCHLIST_SIZE = 50\n",
    "\n",
    "print('BASE_DIR:', BASE_DIR)\n",
    "print('DATA_PATH exists:', os.path.exists(DATA_PATH))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8461582e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Hashing & normalization ---\n",
    "\n",
    "def secure_hash(address: str, salt: str = \"chainguard_secret_key_2025\") -> str:\n",
    "    if pd.isna(address):\n",
    "        return None\n",
    "    return hashlib.sha256((str(address).strip().lower() + salt).encode()).hexdigest()\n",
    "\n",
    "\n",
    "def normalize_hash(h: str) -> str:\n",
    "    if h is None:\n",
    "        return None\n",
    "    return str(h).strip().lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d5f017",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Load Transactions\n",
    "We load the uploaded **`ethereum_sample_data.csv`**. The notebook includes best‑effort header mapping for common variants.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dd24e45",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Input CSV not found at c:\\Users\\pravin sharma\\Downloads\\ethereum_sample_data.csv.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      2\u001b[39m rename_map = {\n\u001b[32m      3\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mblock_timestamp\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      4\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtime\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mblock_timestamp\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mamount\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     11\u001b[39m }\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(DATA_PATH):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInput CSV not found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Read CSV\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Note: values like '0E-9' are valid scientific notation (zero); pd.to_numeric will coerce as needed later\u001b[39;00m\n\u001b[32m     18\u001b[39m raw = pd.read_csv(DATA_PATH)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Input CSV not found at c:\\Users\\pravin sharma\\Downloads\\ethereum_sample_data.csv."
     ]
    }
   ],
   "source": [
    "\n",
    "# Best-effort header mapping (your file already matches these names)\n",
    "rename_map = {\n",
    "    'timestamp': 'block_timestamp',\n",
    "    'time': 'block_timestamp',\n",
    "    'tx_hash': 'transaction_hash',\n",
    "    'hash': 'transaction_hash',\n",
    "    'from': 'from_address',\n",
    "    'to': 'to_address',\n",
    "    'value_wei': 'value',\n",
    "    'amount': 'value'\n",
    "}\n",
    "\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    raise FileNotFoundError(f\"Input CSV not found at {DATA_PATH}.\")\n",
    "\n",
    "# Read CSV\n",
    "# Note: values like '0E-9' are valid scientific notation (zero); pd.to_numeric will coerce as needed later\n",
    "raw = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Apply header mapping only if needed\n",
    "for k, v in rename_map.items():\n",
    "    if k in raw.columns and v not in raw.columns:\n",
    "        raw[v] = raw[k]\n",
    "\n",
    "print(f\"Loaded {len(raw)} rows\")\n",
    "raw.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea015b8f",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Hashing & Feature Extraction\n",
    "We compute the features used by the risk model / heuristic:\n",
    "- `value_eth`, `tx_fee`, `hour`\n",
    "- counts per wallet (`from_address_hashed_tx_count`, `to_address_hashed_tx_count`)\n",
    "- `value_z_score` (sender-side)\n",
    "- `time_since_last_sender_tx`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6e05f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Coerce types safely\n",
    "raw['block_timestamp'] = pd.to_datetime(raw.get('block_timestamp'), errors='coerce', utc=True)\n",
    "raw['value'] = pd.to_numeric(raw.get('value'), errors='coerce')\n",
    "raw['gas'] = pd.to_numeric(raw.get('gas'), errors='coerce')\n",
    "raw['gas_price'] = pd.to_numeric(raw.get('gas_price'), errors='coerce')\n",
    "\n",
    "# Normalize addresses and hash (privacy-preserving)\n",
    "raw['from_address'] = raw.get('from_address', '').astype(str).str.strip().str.lower()\n",
    "raw['to_address'] = raw.get('to_address', '').astype(str).str.strip().str.lower()\n",
    "raw['from_address_hashed'] = raw['from_address'].apply(secure_hash)\n",
    "raw['to_address_hashed'] = raw['to_address'].apply(secure_hash)\n",
    "\n",
    "# Derived features\n",
    "raw = raw.sort_values('block_timestamp').reset_index(drop=True)\n",
    "raw['value_eth'] = raw['value'] / (10**18)\n",
    "raw['tx_fee'] = raw['gas'] * raw['gas_price']\n",
    "raw['hour'] = raw['block_timestamp'].dt.hour\n",
    "\n",
    "# Counts per hashed address\n",
    "for col in ['from_address_hashed', 'to_address_hashed']:\n",
    "    raw[f'{col}_tx_count'] = raw.groupby(col)['block_timestamp'].transform('count')\n",
    "\n",
    "# Sender value history: mean/std → z-score\n",
    "hist = raw.groupby('from_address_hashed')['value_eth'].agg(['mean', 'std'])\n",
    "hist.columns = ['sender_mean', 'sender_std']\n",
    "df_proc = raw.merge(hist, left_on='from_address_hashed', right_index=True, how='left')\n",
    "\n",
    "# Z-score with safe NaN/Inf handling\n",
    "df_proc['value_z_score'] = ((df_proc['value_eth'] - df_proc['sender_mean']) / df_proc['sender_std'])\n",
    "df_proc['value_z_score'] = np.nan_to_num(df_proc['value_z_score'], nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# Time since last sender transaction\n",
    "df_proc['time_since_last_sender_tx'] = (\n",
    "    df_proc.groupby('from_address_hashed')['block_timestamp']\n",
    "           .diff()\n",
    "           .dt.total_seconds()\n",
    "           .fillna(0)\n",
    ")\n",
    "\n",
    "print('Feature matrix shape:', df_proc.shape)\n",
    "df_proc.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08752e5b",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Risk Scoring\n",
    "If a trained model & scaler are present in `./models/`, they will be used. Otherwise we compute a heuristic risk score that still maps to **0..100**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7826e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = [\n",
    "    'value_eth', 'tx_fee', 'hour',\n",
    "    'from_address_hashed_tx_count', 'to_address_hashed_tx_count',\n",
    "    'value_z_score', 'gas', 'gas_price', 'time_since_last_sender_tx'\n",
    "]\n",
    "\n",
    "use_model = os.path.exists(MODEL_PATH) and os.path.exists(SCALER_PATH)\n",
    "print('Model available:', use_model)\n",
    "\n",
    "if use_model:\n",
    "    try:\n",
    "        model = joblib.load(MODEL_PATH)\n",
    "        scaler = joblib.load(SCALER_PATH)\n",
    "        X = df_proc[features].fillna(0).values\n",
    "        Xs = scaler.transform(X)\n",
    "        raw_score = model.decision_function(Xs)\n",
    "        min_s, max_s = np.min(raw_score), np.max(raw_score)\n",
    "        denom = (max_s - min_s) if (max_s - min_s) != 0 else 1.0\n",
    "        risk = 100 * (1 - (raw_score - min_s) / denom)\n",
    "        df_proc['risk_score'] = np.round(risk, 2)\n",
    "    except Exception as e:\n",
    "        print('Model inference failed. Falling back to heuristic:', e)\n",
    "        use_model = False\n",
    "\n",
    "if not use_model:\n",
    "    # Heuristic: combine |z| and normalized fee\n",
    "    z = df_proc['value_z_score'].fillna(0).to_numpy()\n",
    "    fee = df_proc['tx_fee'].fillna(0).to_numpy()\n",
    "    fee_norm = (fee - fee.min()) / (fee.max() - fee.min() + 1e-9)\n",
    "    raw_h = 0.6 * np.abs(z) + 0.4 * fee_norm\n",
    "    raw_norm = (raw_h - raw_h.min()) / (raw_h.max() - raw_h.min() + 1e-9)\n",
    "    df_proc['risk_score'] = np.round(100 * raw_norm, 2)\n",
    "\n",
    "# Preview\n",
    "cols_preview = ['transaction_hash','from_address_hashed','to_address_hashed','value_eth','risk_score','block_timestamp']\n",
    "df_proc[cols_preview].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212ec816",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Classification & Reports\n",
    "We produce the standard artifacts used by the Streamlit app:\n",
    "- `all_scored_transactions.csv`\n",
    "- `top_100_risky_transactions.csv` (ordered columns)\n",
    "- `watchlist_accounts.csv`\n",
    "- `watchlist_deep_dive_report.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672aa5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save ALL scored data\n",
    "df_proc.to_csv(OUT_ALL_SCORED, index=False)\n",
    "print('Saved:', OUT_ALL_SCORED)\n",
    "\n",
    "# Top-100 risky transactions (ordered per UI requirement)\n",
    "report_cols = ['transaction_hash', 'value_eth', 'risk_score', 'block_timestamp', 'from_address_hashed']\n",
    "top_100 = df_proc.sort_values(by='risk_score', ascending=False).head(100)\n",
    "top_100[report_cols].to_csv(OUT_TOP100, index=False)\n",
    "print('Saved:', OUT_TOP100)\n",
    "\n",
    "# Watchlist (count high-risk events across sender & receiver)\n",
    "df_high = df_proc[df_proc['risk_score'] >= RISK_THRESHOLD].copy()\n",
    "sender_counts = df_high['from_address_hashed'].value_counts().reset_index()\n",
    "sender_counts.columns = ['wallet_hash', 'risky_tx_count']\n",
    "receiver_counts = df_high['to_address_hashed'].value_counts().reset_index()\n",
    "receiver_counts.columns = ['wallet_hash', 'risky_tx_count']\n",
    "\n",
    "wallet_risk = pd.concat([sender_counts, receiver_counts]).groupby('wallet_hash').sum().reset_index()\n",
    "watchlist = wallet_risk.sort_values('risky_tx_count', ascending=False).head(WATCHLIST_SIZE)\n",
    "watchlist.to_csv(OUT_WATCHLIST, index=False)\n",
    "print('Saved:', OUT_WATCHLIST)\n",
    "\n",
    "# Deep-Dive: all transactions where sender OR receiver is in watchlist\n",
    "wl = watchlist['wallet_hash'].tolist()\n",
    "deep = df_proc[(df_proc['from_address_hashed'].isin(wl)) | (df_proc['to_address_hashed'].isin(wl))].copy()\n",
    "deep['direction'] = np.where(\n",
    "    deep['from_address_hashed'].isin(wl), 'sent',\n",
    "    np.where(deep['to_address_hashed'].isin(wl), 'received', 'other')\n",
    ")\n",
    "\n",
    "deep.to_csv(OUT_DEEPDIVE, index=False)\n",
    "print('Saved:', OUT_DEEPDIVE)\n",
    "\n",
    "# Quick preview of Top-100\n",
    "top_100[['transaction_hash','risk_score']].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2f3523",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Rich Visualizations for Risk Scores\n",
    "The following plots help explain risk distribution and drivers:\n",
    "- **Histogram & KDE**\n",
    "- **Box/Violin by hour of day**\n",
    "- **Scatter: Value (ETH) vs Risk**\n",
    "- **Time series: Risk over time**\n",
    "- **Correlation heatmap**\n",
    "- **Top wallets by avg risk**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d274d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "risk = pd.to_numeric(df_proc['risk_score'], errors='coerce')\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.hist(risk.dropna(), bins=30, color='#ef4444', alpha=0.85, edgecolor='white')\n",
    "plt.title('Risk Score Histogram')\n",
    "plt.xlabel('Risk Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.25)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd515fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# KDE (if seaborn available)\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    plt.figure(figsize=(7,4))\n",
    "    sns.kdeplot(risk.dropna(), fill=True, color='#ef4444')\n",
    "    plt.title('Risk Score Density (KDE)')\n",
    "    plt.xlabel('Risk Score')\n",
    "    plt.ylabel('Density')\n",
    "    plt.grid(True, alpha=0.2)\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print('Seaborn not available or KDE failed:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9750b1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Boxplot & Violin by hour\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    df_plot = df_proc[['hour','risk_score']].dropna()\n",
    "    df_plot['hour'] = pd.to_numeric(df_plot['hour'], errors='coerce')\n",
    "\n",
    "    plt.figure(figsize=(10,4))\n",
    "    sns.boxplot(data=df_plot, x='hour', y='risk_score', palette='Reds')\n",
    "    plt.title('Risk Score by Hour of Day (Boxplot)')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('Risk Score')\n",
    "    plt.grid(True, axis='y', alpha=0.2)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10,4))\n",
    "    sns.violinplot(data=df_plot, x='hour', y='risk_score', palette='Reds')\n",
    "    plt.title('Risk Score by Hour of Day (Violin)')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('Risk Score')\n",
    "    plt.grid(True, axis='y', alpha=0.2)\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print('Seaborn not available for box/violin:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c443aaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Scatter: Value (ETH) vs Risk\n",
    "val = pd.to_numeric(df_proc['value_eth'], errors='coerce')\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(val, risk, s=10, alpha=0.6, c=risk, cmap='Reds')\n",
    "plt.title('Value (ETH) vs Risk')\n",
    "plt.xlabel('Value (ETH)')\n",
    "plt.ylabel('Risk Score')\n",
    "plt.grid(True, alpha=0.25)\n",
    "plt.colorbar(label='Risk Score')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ad651e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Time series: Risk over time\n",
    "ts = pd.to_datetime(df_proc['block_timestamp'], errors='coerce')\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(ts, risk, color='#ef4444', alpha=0.7)\n",
    "plt.title('Risk Over Time')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Risk Score')\n",
    "plt.grid(True, alpha=0.25)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a1ca86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Correlation heatmap\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    feat_cols = [\n",
    "        'value_eth','tx_fee','hour',\n",
    "        'from_address_hashed_tx_count','to_address_hashed_tx_count',\n",
    "        'value_z_score','gas','gas_price','time_since_last_sender_tx','risk_score'\n",
    "    ]\n",
    "    df_corr = df_proc[feat_cols].copy().apply(pd.to_numeric, errors='coerce')\n",
    "    corr = df_corr.corr()\n",
    "    plt.figure(figsize=(9,7))\n",
    "    sns.heatmap(corr, cmap='Reds', annot=True, fmt='.2f', square=True)\n",
    "    plt.title('Feature Correlation Heatmap')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print('Seaborn not available for heatmap:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ecd3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Top wallets by average risk (sender-side)\n",
    "rank = df_proc.groupby('from_address_hashed')['risk_score'].mean().sort_values(ascending=False).head(10)\n",
    "print('Top 10 wallets by average risk (sender-side):')\n",
    "print(rank)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc53c873",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### Notes\n",
    "- If trained model files are not present, heuristic fallback is applied automatically.\n",
    "- The generated CSVs are compatible with your Streamlit dashboard.\n",
    "- You can safely re-run sections to iterate quickly during demos.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_tf_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
